{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from loguru import logger\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "from PIL import Image\n",
    "import gradio as gr\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from PIL import Image\n",
    "from uuid import uuid1\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NEW_TOKENS = 200\n",
    "THRESHOLD = 0.01 # threshold of retrieval score\n",
    "IMG_ROOT_PATH = \"data/\"\n",
    "os.makedirs(IMG_ROOT_PATH, exist_ok=True)\n",
    "os.environ['OPENAI_API_KEY'] = \"\" #Input your openai api key here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"bczhou/tiny-llava-v1-hf\"\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    model_id, low_cpu_mem_usage=True, device_map=\"cuda\"\n",
    ").eval()# type:ignore\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "# Use OpenAI's embeddings for our Chroma collection.\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),  # type: ignore\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This generate a persistant collection. If you want to clear all cached information, delete 'data' in the same directory.\n",
    "# collection = Chroma(\"conversation_memory\", embeddings, persist_directory=f'{IMG_ROOT_PATH}/chroma')\n",
    "\n",
    "# This generate a temporary collection.\n",
    "collection = Chroma(\"conversation_memory\", embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(image: Image.Image, message: str, **kwargs):\n",
    "    prompt = f\"USER: <image>\\n{message}\\nASSISTANT:\"\n",
    "    logger.info(f\" ==== prompt ====\\n{message}\")\n",
    "    inputs = processor(prompt, image, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, **kwargs)\n",
    "    texts = [processor.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "    responses = [\n",
    "        {\"content\": text.split(\"ASSISTANT:\")[-1].strip(), \"index\": i}\n",
    "        for i, text in enumerate(texts)\n",
    "    ]\n",
    "    logger.info(f\" ==== responses ====\\n{responses}\")\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(\n",
    "    user_input: str, img: np.ndarray\n",
    ") -> Tuple[str, str, Optional[np.ndarray]]:\n",
    "    img_id = str(uuid1())\n",
    "    img_save_pth = Path(f\"data/{img_id}.jpeg\")\n",
    "    img_obj = Image.fromarray(img)\n",
    "    img_obj.save(img_save_pth)\n",
    "    # generate image description\n",
    "    query = \"Describe the image in detail.\"\n",
    "    responses = generate_response(\n",
    "        img_obj, query, max_new_tokens=MAX_NEW_TOKENS, do_sample=True, temperature=0.7\n",
    "    )\n",
    "    img_desc = responses[0][\"content\"]\n",
    "\n",
    "    # find related image information from collection\n",
    "    docs_with_score = collection.similarity_search_with_score(query=img_desc, k=1)\n",
    "    logger.info(f\" ==== find from collection ====\\n{docs_with_score}\")\n",
    "    related_img_info_lst=[]\n",
    "    related_img_objs=[]\n",
    "    if len(docs_with_score) > 0:\n",
    "        docs_with_score = sorted(\n",
    "            docs_with_score,\n",
    "            reverse=True,\n",
    "            key = lambda x:x[1]\n",
    "        )\n",
    "        for doc, score in docs_with_score:\n",
    "            if score>=THRESHOLD:\n",
    "                related_img_info_lst.append(\n",
    "                    f\"**description:**\\n{doc.page_content}\"\n",
    "                )\n",
    "                related_img_objs.append(\n",
    "                    Image.open(doc.metadata[\"path\"])\n",
    "                )\n",
    "    if related_img_info_lst:\n",
    "        related_img_info = related_img_info_lst[0]\n",
    "        related_img_obj = related_img_objs[0]\n",
    "    else:\n",
    "        related_img_info = \"No related image found.\"\n",
    "        related_img_obj = None\n",
    "    \n",
    "    # add image description to collection\n",
    "    collection.add_texts(\n",
    "        texts=[img_desc], metadatas=[{\"path\": str(img_save_pth.absolute())}]\n",
    "    )\n",
    "    logger.debug(f\" ==== add texts to vector storage ====\\n{img_desc}\")\n",
    "\n",
    "    # Visual question answer\n",
    "    responses = generate_response(\n",
    "        img_obj,\n",
    "        user_input,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    response = responses[0][\"content\"]\n",
    "\n",
    "    # Return model output.\n",
    "    return (\n",
    "        f\"# Response\\n{response.strip()}\",\n",
    "        f\"# Related Image Infomation\\n{related_img_info.strip()}\",\n",
    "        np.array(related_img_obj) if related_img_obj else None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = gr.Interface(\n",
    "    generate_output,\n",
    "    inputs=[\n",
    "        gr.Textbox(\n",
    "            label=\"Input text\",\n",
    "            scale=7,\n",
    "        ),\n",
    "        gr.Image(label=\"Input image\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Markdown(value=\"# Response\", line_breaks=True),\n",
    "        gr.Markdown(value=\"# Related Image Infomation\", line_breaks=True),\n",
    "        gr.Image(label=\"related image\"),\n",
    "    ],\n",
    "    clear_btn=gr.Button(\"Clear\"),\n",
    "    title=\"TinyLLaVA language-Image QA demo\",\n",
    "    description=\"\"\"\n",
    "        **IMPORTANT:** You need to provide both an image and a text-based query.\n",
    "        \"\"\",\n",
    "    allow_flagging=\"never\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.queue().launch(server_port=7860)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinyllava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
