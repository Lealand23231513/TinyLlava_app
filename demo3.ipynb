{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "from loguru import logger\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "from PIL import Image\n",
    "import gradio as gr\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from PIL import Image\n",
    "from uuid import uuid3, NAMESPACE_DNS\n",
    "from pathlib import Path\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NEW_TOKENS = 200\n",
    "IMG_ROOT_PATH = \"data/\"\n",
    "THRESHOLD = 0.01\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = \"\" #Input your openai api key here\n",
    "\n",
    "model_id = \"bczhou/tiny-llava-v1-hf\"\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    model_id, low_cpu_mem_usage=True, device_map=\"cuda\"\n",
    ").eval()  # type:ignore\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "# Use OpenAI's embeddings for our Chroma collection.\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),  # type: ignore\n",
    ")\n",
    "\n",
    "## This generate a persistant collection. If you want to clear all cached information, delete 'data' in the same directory.\n",
    "# collection = Chroma(\"conversation_memory\", embeddings, persist_directory=f'{IMG_ROOT_PATH}/chroma')\n",
    "\n",
    "# This generate a temporary collection.\n",
    "collection = Chroma(\"conversation_memory\", embeddings)\n",
    "os.makedirs(IMG_ROOT_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(image: Image.Image, message: str, **kwargs):\n",
    "    '''\n",
    "    This auxiliary function is used to generate module response based on the given image and user's query.\n",
    "    '''\n",
    "    prompt = f\"USER: <image>\\n{message}\\nASSISTANT:\"\n",
    "    logger.info(f\" ==== prompt ====\\n{message}\")\n",
    "    inputs = processor(prompt, image, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, **kwargs)\n",
    "    texts = [processor.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "    responses = [\n",
    "        {\"content\": text.split(\"ASSISTANT:\")[-1].strip(), \"index\": i}\n",
    "        for i, text in enumerate(texts)\n",
    "    ]\n",
    "    logger.info(f\" ==== responses ====\\n{responses}\")\n",
    "    return responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(conversation:list, user_input: str, img_obj):\n",
    "    conversation.append([user_input, None])\n",
    "    conversation.append([(img_obj.name,), None])\n",
    "    yield conversation\n",
    "    img_id = str(uuid3(NAMESPACE_DNS, Path(img_obj.name).name))\n",
    "    img_save_pth = Path(f\"data/{img_id}.jpeg\")\n",
    "    shutil.copy(img_obj.name, img_save_pth)\n",
    "    # generate image description\n",
    "    query = \"Describe the image in detail.\"\n",
    "    responses = generate_response(\n",
    "        Image.open(img_save_pth),\n",
    "        query,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    img_desc = responses[0][\"content\"]\n",
    "\n",
    "    # find related image information from collection\n",
    "    docs_with_score = collection.similarity_search_with_score(query=img_desc, k=1)\n",
    "    logger.info(f\" ==== find from collection ====\\n{docs_with_score}\")\n",
    "    related_doc_info = []\n",
    "    if len(docs_with_score) > 0:\n",
    "        docs_with_score = sorted(docs_with_score, reverse=True, key=lambda x: x[1])\n",
    "        for doc, score in docs_with_score:\n",
    "            if score >= THRESHOLD:\n",
    "                related_doc_info.append(doc)\n",
    "    if related_doc_info:\n",
    "        most_related_doc_info = related_doc_info[0]\n",
    "    else:\n",
    "        most_related_doc_info = None\n",
    "\n",
    "    # Add image description to collection\n",
    "    collection.add_texts(\n",
    "        texts=[img_desc], metadatas=[{\"path\": str(img_save_pth.absolute())}]\n",
    "    )\n",
    "    logger.debug(f\" ==== add texts to vector storage ====\\n{img_desc}\")\n",
    "\n",
    "    # Visual question answer\n",
    "    responses = generate_response(\n",
    "        Image.open(img_save_pth),\n",
    "        user_input,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    response = responses[0][\"content\"]\n",
    "\n",
    "    # Yield model output for given query and image.\n",
    "    conversation.append([None, response.strip()])\n",
    "    yield conversation\n",
    "    # Yield model output for related image\n",
    "    if most_related_doc_info:\n",
    "        conversation.append([None, \"I found a similar image:\"])\n",
    "        conversation.append([None, (most_related_doc_info.metadata[\"path\"],)])\n",
    "        yield conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            chatbot = gr.Chatbot(height=800)\n",
    "        with gr.Column():\n",
    "            with gr.Row():\n",
    "                text_box = gr.Textbox(\n",
    "                    lines=10,\n",
    "                    scale=7,\n",
    "                    placeholder=\"Enter text and upload an image, press the button to submit\"\n",
    "                )\n",
    "                image_box = gr.File(scale=3, file_types=[\"image\"])\n",
    "            with gr.Row():\n",
    "                submit_btn = gr.Button(\"submit\")\n",
    "                clear_btn = gr.ClearButton(\n",
    "                    components=[chatbot, text_box, image_box]\n",
    "                )\n",
    "    submit_btn.click(\n",
    "        fn=generate_output,\n",
    "        inputs=[chatbot, text_box, image_box],\n",
    "        outputs=[chatbot],\n",
    "    )\n",
    "demo.launch(server_port=7860)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinyllava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
